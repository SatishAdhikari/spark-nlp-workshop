{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workshop Goals\n",
    "\n",
    "### - Get to know Apache Spark engine.\n",
    "\n",
    "### - Understand Spacy NLP library capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apache Spark is a fast and general engine for large-scale data processing\n",
    "![Spark Libs](img/spark-libs.png)\n",
    "\n",
    "### It can access diverse data sources including HDFS, Cassandra, Hive, HBase, S3 and JDBC/ODBC\n",
    "![Spark Compatabilities](img/spark-cmp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Hadoop data sharing](img/data-sharing-mapreduce.png)\n",
    "![Spark data sharing](img/data-sharing-spark.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as fun, types\n",
    "\n",
    "import spacy\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth', 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark session init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession(SparkContext.getOrCreate()) \\\n",
    "    .builder \\\n",
    "    .appName('NLP') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "News Category Dataset:\n",
    "https://www.kaggle.com/rmisra/news-category-dataset\n",
    "\n",
    "Each json record contains following attributes:\n",
    "\n",
    "* category: Category article belongs to\n",
    "\n",
    "* headline: Headline of the article\n",
    "\n",
    "* authors: Person authored the article\n",
    "\n",
    "* link: Link to the post\n",
    "\n",
    "* short_description: Short description of the article\n",
    "\n",
    "* date: Date the article was published"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = spark.read.json(\"News_Category_Dataset_v2.json\")\n",
    "news_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.createOrReplaceTempView(\"news\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT COUNT(*) AS count FROM news\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT category, count(category) AS count FROM news GROUP BY category ORDER BY count DESC\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.groupby('category') \\\n",
    "    .count() \\\n",
    "    .orderBy(fun.desc('count')) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1. Select the longest headline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Use `length` function and `LIMIT` expression in SQL\n",
    "\n",
    "Available functions: http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#module-pyspark.sql.functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy NLP library\n",
    "![Spacy Features](img/spacy-features.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy \n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"In 2018 the Debian Linux project received a donation of $300,000\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc.noun_chunks:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    if token.like_num:\n",
    "        print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2. Extract named entities from the string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Use `ents` attribute of the `Doc` and `label_` attribute of the `Token`\n",
    "\n",
    "Spacy Cheat Sheet: http://datacamp-community-prod.s3.amazonaws.com/29aa28bf-570a-4965-8f54-d6a541ae4e06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's combine a power of these two instruments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3. Extract ORG, PERSON, GPE named entities in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Write a function that takes a news headline and generate the output like that\n",
    "\n",
    "[\n",
    "  {\n",
    "    'label': 'ORG', \n",
    "    'text': 'ACME Inc.'\n",
    "  },\n",
    "  {\n",
    "    'label': 'PERSON', \n",
    "    'text': 'John Doe'   \n",
    "  },\n",
    "  {\n",
    "    'label': GPE,\n",
    "    'text': 'London'\n",
    "  }\n",
    "  ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df_sample = news_df.sample(withReplacement=False, fraction=0.002, seed=777)\n",
    "news_df_sample.createOrReplaceTempView(\"news_sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpacyWrapper(object):\n",
    "    \"\"\"Wrapper class to load Spacy on worker nodes\"\"\"\n",
    "    _spacys = {}\n",
    "    disabled_pipeline_steps = ['parser', 'tagger']\n",
    "    default_model = 'en_core_web_sm'\n",
    "\n",
    "    @classmethod\n",
    "    def get(cls, model=default_model, disable=disabled_pipeline_steps):\n",
    "        if model not in cls._spacys:\n",
    "            import spacy\n",
    "            cls._spacys[model] = spacy.load(model, disable=disable)\n",
    "        return cls._spacys[model]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neamed entity extraction function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Reuse the code from `Task 2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner(doc):\n",
    "    labels=['ORG', 'PERSON', 'GPE']\n",
    "    entities = []\n",
    "    \n",
    "    # Load Spacy\n",
    "    nlp = SpacyWrapper.get()\n",
    "    doc = nlp(doc)\n",
    "    \n",
    "    # ======== WRITE YOUR SOLUTION BELOW ======== \n",
    "        \n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema definition\n",
    "schema = types.ArrayType(\n",
    "    types.StructType([\n",
    "        types.StructField('label', types.StringType(), nullable=False),\n",
    "        types.StructField('text', types.StringType(), nullable=False)\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Register user defined function (UDF) to use in SQL\n",
    "spark.udf.register('ner', ner, schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply UDF to extract headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_sample = spark.sql(\"SELECT short_description, ner(short_description) AS entities FROM news_sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_sample.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save output to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT short_description, ner(short_description) AS entities FROM news_sample\") \\\n",
    "    .repartition(1) \\\n",
    "    .write \\\n",
    "    .json(\"output\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
